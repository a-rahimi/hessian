\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{authblk}

\geometry{margin=1in}

\title{The Hessian of a Deep Net}
\author{Ali Rahimi}

\begin{document}

\maketitle

\section{Introduction}

The Hessian of a deep net is the matrix of second-order mixed partial
derivatives of its loss with respect to the parameters. Equivalently, it's the
derivative of the gradient with respect to the parameters. Over the past few
decades, relying on this Hessian has come in and out of favor. In the 80s, when
deep nets had only thousands of parameters, the Hessian could be used to
implement second order optimizers that converged must faster than gradient
descent \cite{deep-net-second-order}. As the number of parameters in deep nets
grew, approximations of the Hessian were employed: low-rank estimates used in
LBFGS \cite{deep-net-lbfg}, and diagonal approximations of the Hessian used in
Adagrad \cite{adagrad}. Methods have also been proposed to approximate the
spectrum of the Hessian to glean insights about the behavior of training
algorithms \cite{ying-behrooz-hessian-spectrum}. However, as modern deep nets
get larger, these methods have become increasingly less practical to apply: for
a model with a billion parameters, the Hessian would have a quintillion
entries, which far more than can be stored, multiplied, factorized, or
inverted, even in the largest data centers.

We show that the Hessian of a deep net has a regular structure that makes it
more amenable to these operations. Regardless of the specific operations in
each layer, the Hessian can be represented as a matrix polynomial that involves
the first order and second order mixed derivatives of each layer. This
polynomial is also second order in the inverse of of a block-bi-diagonal
operator that represents the backpropoagatin algorithm. This structure allows
us to multiply the Hessian by a vector, or solve linear systems involving the
Hessian without ever forming or storing the full matrix. For an $L$-layer deep
net where each layer has $p$ parameters and produces $a$ activations, naively
storing the Hessian would require $O(L^2p^2)$ memory, and multiplying it by a
vector or solving a linear system would require $O(L^2p^2 + L\max(a,p)^3)$ and
$O(L^3p^3)$ operations, respectively. In contrast, we will show how to perform
these operations using only $O(L \max(a, p)^3)$ computations. The dependence on
the parameters on the number of activations and parameters in each layer is
still cubic, but the dependence on the number of layers is only linear. This
makes operating on the Hessian of tall and skinny networks more efficient than
the Hessian of short and fat networks.

\section{Overview}

Our objective is to efficiently solve linear systems of the form $H x = g$,
where $H$ is the Hessian of a deep neural network. Forming $H$ explicitly is
infeasible due to its size, and directly inverting or multiplying by $H$ would
require $O(L^3p^3)$ and $O(L^2p^2)$ operations, respectively---both
prohibitively expensive for large networks. To overcome this, we employ the
following strategy:
\begin{enumerate}
    \item
          Write down the gradient of the deep net as a bi-diagonal system of linear
          equations. Solving this system uses back-substitution, which requires a forward
          and backward pass similar to those used in backpropagation. In fact,
          back-substitution and backpropagation are identical in this context.
    \item
          Differentiate the components of this linear system, including the bi-diagonal
          matrix itself, to obtain the second-order derivatives.
    \item
          Reformulate the resulting expressions so that the Hessian appears as a
          second-order polynomial in the inverse of the bi-diagonal matrix.
    \item
          Derive a fast algorithm to apply the inverse of such polynomials.
\end{enumerate}

\section{Notation}

We write a deep net as a pipeline of functions $\ell = 1, \ldots, L$,
\begin{align}
    z_1    & = f_1(z_0; x_1) \nonumber              \\
           & \ldots \nonumber                       \\
    z_\ell & = f_\ell(z_{\ell-1}; x_\ell) \nonumber \\
           & \ldots \nonumber                       \\
    z_L    & = f_L(z_{L-1}; x_L)
\end{align}

The vectors $x_1,\ldots, x_L$ are the parameters of the pipeline. The vectors
$z_1,\ldots, z_L$ are its intermediate activations, and $z_0$ is the input to
the pipeline. The last layer $f_L$ computes the final activations and their
training loss, so the scalar $z_L$ is the loss of the model on the input $z_0$.
To make this loss's dependence on $z_0$ and the parameters explicit, we
sometimes write it as $z_L(z_0;x)$. This formalization deviates slightly from
the traditional deep net formalism in two ways: First, the training labels are
subsumed in $z_0$, and are propagated through the layers until they're used in
the loss. Second, the last layer fuses the loss (which has no parameters) and
the last layer (which does).

\section{Backpropagation, the Matrix Way}

We would like to fit the vector of parameters $x = (x_1, \ldots, x_L)$ given a
training dataset, which we represent by a stochastic input $z_0$ to the
pipeline. Training the model proceeds by gradient descent steps along the
stochastic gradient $\partial z_L(z_0;x) / \partial x$. The components of this
direction can be computed by the chain rule with a backward recursion:
\begin{align*}
    \frac{\partial z_L}{\partial x_\ell} & = \underbrace{\frac{\partial z_L}{\partial z_{\ell}}}_{b_\ell} \underbrace{\frac{\partial z_\ell}{\partial x_\ell}}_{\nabla_x f_\ell}               \\
    \frac{\partial z_L}{\partial z_\ell} & = \underbrace{\frac{\partial z_L}{\partial z_{\ell+1}}}_{b_{\ell+1}} \underbrace{\frac{\partial z_{\ell+1}}{\partial z_\ell}}_{\nabla_z f_{\ell+1}}
\end{align*}
The identification $b_\ell \equiv \frac{\partial z_L}{\partial z_\ell}$, $\nabla_x f_\ell \equiv \frac{\partial z_\ell}{\partial x_\ell}$, and $\nabla_z f_\ell \equiv \frac{\partial z_\ell}{\partial z_{\ell-1}}$ turns this recurrence into
\begin{align*}
    \frac{\partial z_L}{\partial x_\ell} & = b_\ell \cdot \nabla_x f_\ell          \\
    b_\ell                               & = b_{\ell+1} \cdot \nabla_z f_{\ell+1},
\end{align*}
with the base case $b_L = 1$, a scalar.

The above equations can be written in vector form as
\begin{equation}
    \frac{\partial z_L}{\partial x} =
    \begin{bmatrix}
        \frac{\partial z_L}{\partial x_1} & \cdots & \frac{\partial z_L}{\partial x_L}
    \end{bmatrix}
    =
    \underbrace{\begin{bmatrix}
            b_1 & \cdots & b_L
        \end{bmatrix}
    }_{\equiv b}
    \underbrace{\begin{bmatrix}
            \nabla_x f_1 &                       \\
                         & \ddots                \\
                         &        & \nabla_x f_L
        \end{bmatrix}
    }_{\equiv D}
\end{equation}
and
\begin{equation}
    \begin{bmatrix}
        b_1 & b_2 & b_3 & \cdots & b_{L-1} & b_L
    \end{bmatrix}
    \underbrace{
        \begin{bmatrix}
            I                                                          \\
            -\nabla_z f_2 & I                                          \\
                          & -\nabla_z f_3 & I                          \\
                          &               & \ddots & \ddots            \\
                          &               &        & -\nabla_z f_L & 1 \\
        \end{bmatrix}
    }_{\equiv M}
    =
    \underbrace{\begin{bmatrix}
            0 & \cdots & 1
        \end{bmatrix}
    }_{\equiv e_L}.
\end{equation}

Solving for $b$ and substituting back gives
\begin{equation}
    \frac{\partial z_L}{\partial x} =  e_L M^{-1} D.
\end{equation}

The matrix $M$ is block bi-diagonal. Its diagonal entries are identity
matrices, and its off-diagonal matrices are the gradient of the intermediate
activations with respect to the layer's parameters. The matrix $D$ is block
diagonal, with the block as the derivative of each layer's activations with
respect to its inputs. $M$ is invertible because the spectrum of a triangular
matrix can be read off its diagonal, which in this case is all ones.

\section{The Hessian}

The gradient we computed above is the unique vector $v$ such that $d z_L \equiv
    z_L(x+dx) - z_L(dx) \to v(x) \cdot dx$ as $dx\to 0$. In this section, we
compute the Hessian $H$ of $z_L$ with respect to the parameters. This is the
unique matrix $H(x)$ such that $dv^\top \equiv v^\top(x+dx) - v^\top(x) \to
    H(x)\; dx$ as $dx \to 0$. We use the facts that $dM^{-1} = -M^{-1} (dM) M^{-1}$
and $b=e_L M^{-1}$ to write
\begin{align}
    dv & = d(e_L M^{-1} D) \nonumber                             \\
       & = e_L M^{-1} (dD)+ e_L \left(dM^{-1}\right) D \nonumber \\
       & = b \cdot dD  - e_L M^{-1} (dM) M^{-1} D \nonumber      \\
       & = b \cdot dD  - b \cdot (dM) M^{-1} D
\end{align}

We compute each of these terms separately.

As part of this agenda, we will rely on the gradient of tensor-valued functions
$g:\mathbb{R}^d \to \mathbb{R}^{o_1 \times \cdots \times o_k}$. Define this
gradient $\nabla_x g(x)\in \mathbb{R}^{(o_1 \cdots o_k) \times d}$ as the
unique matrix-valued function that satisfies \begin{equation}
    \mathrm{vec}
    \left(g(x+dx) - g(x)\right) \to \nabla_x g(x) \cdot dx
\end{equation}
as $dx \to
    0$.  This convention readily implies the Hessian of a vector-valued function: If
$g:\mathbb{R}^d \to \mathbb{R}^o$, then $\nabla_{xx} g(x) \in
    \mathbb{R}^{o\times d^2}$ is the unique matrix such that $\mathrm{vec}
    \left(\nabla_x g(x+dx) - \nabla_x g(x)\right) \to \nabla_{xx} g(x) \; dx$. This
convention also readily accommodates the chain rule. For example, the gradient
of $h(x) \equiv f(g(x))$ for matrix-valued $f$ and $g$ can be written as $\nabla
    f \nabla g$ as expected. It also implies partial derivatives like $\nabla_{yz}
    g$ for $g:\mathbb{R}^{|x|} \to \mathbb{R}^{|g|}$.  If $y\in \mathbb{R}^{|y|}$
and $z\in \mathbb{R}^{|z|}$ are restrictions of $x \in \mathbb{R}^{|x|}$ to some
$|y|$ and $|z|$-dimensional subsets, then $\nabla_z g(x) \in \mathbb{R}^{|g|
        \times |z|}$, and $\nabla_{yz} g(x) = \nabla_y \nabla_z g(x) \in
    \mathbb{R}^{|g||z| \times |y|}$. See Chapter 6 of Magnus \& Neudecker
\cite{magnus-neudecker} for a deeper treatment of higher order derivatives of
vector-valued functions.

\subsection{The Term Involving $dD$}

The matrix $D$ is block-diagonal with its $\ell$th diagonal block containing
the matrix $D_\ell \equiv \nabla_x f_\ell$. Using the facts that
$\mathrm{vec}(ABC) = \left(C^\top \otimes A \right) \mathrm{vec}(B)$, and
$(A\otimes B)^\top = A^\top \otimes B^\top$, we get
\begin{align}
    b\cdot (dD) & =
    \begin{bmatrix}
        b_1 & \cdots & b_L
    \end{bmatrix}
    \begin{bmatrix}
        dD_1 &        &      \\
             & \ddots &      \\
             &        & dD_L
    \end{bmatrix}
    \nonumber                                                                            \\
                & = \begin{bmatrix}
                        b_1 \cdot dD_1 & \cdots & b_L \cdot dD_L
                    \end{bmatrix}
    \nonumber                                                                            \\
                & = \begin{bmatrix}
                        \mathrm{vec}\left(dD_1\right)^\top \left(I \otimes b_1^\top\right) &
                        \cdots                                                             &
                        \mathrm{vec}\left(dD_L\right)^\top \left(I \otimes b_L^\top\right)
                    \end{bmatrix}
    \nonumber                                                                            \\
                & = \begin{bmatrix}
                        \mathrm{vec}\left(dD_1\right) \\
                        \vdots                        \\
                        \mathrm{vec}\left(dD_L\right)
                    \end{bmatrix}
    ^\top
    \begin{bmatrix}
        I \otimes b_1^\top &        &                    \\
                           & \ddots &                    \\
                           &        & I \otimes b_L^\top
    \end{bmatrix}
\end{align}

Observe that $\mathrm{vec}\left(dD_\ell\right) = d\,\mathrm{vec} \nabla_x
    f_\ell(z_{\ell-1}; x_\ell)$ varies with $dx$ through both its arguments
$x_\ell$ and $z_{\ell-1}$. Using mixed partials of vector-valued functions
described above, we get
\begin{equation}
    \mathrm{vec} \left(dD_\ell\right) = d\,\mathrm{vec}\left(\nabla_x f_\ell\right) = \left(\nabla_{xx} f_\ell\right)\; dx_\ell  + \left(\nabla_{zx} f_\ell\right)\; dz_{\ell-1}.
\end{equation}

Stacking these equations gives
\begin{align}
    \begin{bmatrix}
        \mathrm{vec}\left(dD_1\right) \\
        \vdots                        \\
        \mathrm{vec}\left(dD_L\right)
    \end{bmatrix}
     & =
    \begin{bmatrix}
        \nabla_{xx} f_1 &        &                 \\
                        & \ddots &                 \\
                        &        & \nabla_{xx} f_L
    \end{bmatrix}
    dx
    +
    \begin{bmatrix}
        \nabla_{zx} f_1 &        &                 \\
                        & \ddots &                 \\
                        &        & \nabla_{zx} f_L
    \end{bmatrix}
    \begin{bmatrix}
        dz_0 \\ \vdots \\ dz_{L-1}
    \end{bmatrix}
    .
\end{align}

Each vector $dz_\ell$ in turn varies with $dx$ via $dz_\ell = (\nabla_x f_\ell)
    dx_\ell + (\nabla_z f_\ell) dz_{\ell-1}$, with the base case $dz_0 = 0$, since
the input $z_0$ does not vary with $dx$. Stacking up this recurrence gives
\begin{align}
    \begin{bmatrix}
        I             &   &                   \\
        -\nabla_z f_2 & I &                   \\
                      &   & \ddots            \\
                      &   & -\nabla_z f_L & 1 \\
    \end{bmatrix}
    \begin{bmatrix}
        dz_1 \\ \vdots \\ dz_{L-1}  \\ dz_L
    \end{bmatrix}
    =
    \begin{bmatrix}
        \nabla_x f_1 &        &              \\
                     & \ddots &              \\
                     &        & \nabla_x f_L
    \end{bmatrix}
    dx.
\end{align}
We can solve for the vector $\begin{bmatrix}
        dz_1 \\ \vdots \\ dz_L
    \end{bmatrix}
    = M^{-1} D dx$ and use the downshifting matrix
\begin{equation}
    P \equiv \begin{bmatrix}
        0 \\ I & 0 \\ &\ddots \\ &I&0
    \end{bmatrix}
\end{equation}
to plug back the vector $\begin{bmatrix}
        dz_0 \\ \vdots \\ dz_{L-1}
    \end{bmatrix}
    =PM^{-1}D dx$:
\begin{align}
    \begin{bmatrix}
        \mathrm{vec}\left(dD_1\right) \\
        \vdots                        \\
        \mathrm{vec}\left(dD_L\right)
    \end{bmatrix}
    =
    \left(
    \begin{bmatrix}
        \nabla_{xx} f_1 &        &                 \\
                        & \ddots &                 \\
                        &        & \nabla_{xx} f_L
    \end{bmatrix}
    +
    \begin{bmatrix}
        \nabla_{zx} f_1 \\ &\ddots& \\ &&\nabla_{zx} f_L
    \end{bmatrix}
    P M^{-1} D
    \right)dx.
\end{align}

\subsection{The Term Involving $dM$}

The matrix $dM$ is lower-block-diagonal with $dM_2,\ldots, dM_L$, and $dM_\ell
    \equiv d \nabla_z f_\ell$. Similar to the above, we can write
\begin{align}
    b & \cdot (dM) M^{-1} D =
    \begin{bmatrix}
        b_1 & \cdots & b_{L-1} & b_L
    \end{bmatrix}
    \begin{bmatrix}
        0                  \\
        -dM_2 & 0          \\
              & \ddots     \\
              & -dM_L  & 0 \\
    \end{bmatrix}
    M^{-1} D \nonumber                                                           \\
      & = -\begin{bmatrix}
               b_2 \cdot dM_2 & \cdots & b_L \cdot dM_L & 0
           \end{bmatrix}
    M^{-1} D \nonumber                                                           \\
      & = -\begin{bmatrix}
               \mathrm{vec} \left(dM_2\right)^\top \left(I \otimes b_2^\top\right) &
               \cdots                                                              &
               \mathrm{vec} \left(dM_L\right)^\top \left(I \otimes b_L^\top\right) &
               0
           \end{bmatrix}
    M^{-1} D \nonumber                                                           \\
      & =
    -\begin{bmatrix}
         \mathrm{vec} \left(dM_1\right) \\
         \vdots                         \\
         \mathrm{vec} \left(dM_L\right)
     \end{bmatrix}
    ^\top
    \begin{bmatrix}
        0                                               \\
        I \otimes b_2^\top & 0                          \\
                           &   & \ddots                 \\
                           &   & I \otimes b_L^\top & 0
    \end{bmatrix}
    M^{-1} D \nonumber                                                           \\
      & = -\begin{bmatrix}
               \mathrm{vec} \left(dM_1\right) \\
               \vdots                         \\
               \mathrm{vec} \left(dM_L\right)
           \end{bmatrix}
    ^\top
    \begin{bmatrix}
        I \otimes b_1^\top \\ &\ddots  \\ && I \otimes b_L^\top
    \end{bmatrix}
    PM^{-1} D.
\end{align}

Each matrix $dM_\ell = d \nabla_z f_\ell(z_{\ell-1}; x_\ell)$ varies with $dx$
through both $x_\ell$ and $z_{\ell -1}$ as $d\,\mathrm{vec} \left(M_\ell\right)
    = \left(\nabla_{xz} f_\ell\right) dx_\ell + \left(\nabla_{zz} f_\ell\right)
    dz_{\ell-1}$. Following the steps of the previous section gives
\begin{align}
    \begin{bmatrix}
        \mathrm{vec}\left(dM_1\right) \\
        \vdots                        \\
        \mathrm{vec}\left(dM_L\right)
    \end{bmatrix}
    =
    \left(
    \begin{bmatrix}
        \nabla_{xz} f_1 &        &                 \\
                        & \ddots &                 \\
                        &        & \nabla_{xz} f_L
    \end{bmatrix}
    +
    \begin{bmatrix}
        \nabla_{zz} f_1 \\ &\ddots& \\ && \nabla_{zz} f_L
    \end{bmatrix}
    P M^{-1} D
    \right)dx.
\end{align}

\subsection{Putting it all Together}

We have just shown that the Hessian of the deep net has the form
\begin{align}
    H \equiv \frac{\partial^2 z_L}{\partial x^2}
     & = D_D \left(D_{xx} + D_{zx} PM^{-1} D_x\right) + D_x^\top M^{-T}P^\top D_M \left(D_{xz}+D_{zz}P M^{-1}D_x\right)           \\
     & = D_DD_{xx}  + D_DD_{zx} PM^{-1} D_x + D_x^\top M^{-\top}P^\top D_M D_{xz}+D_x^\top M^{-\top}P^\top D_M D_{zz}P M^{-1}D_x.
\end{align}

The definitions below annotate the size of the various matrices in this
expression assuming all but the last $L$ layers have $a$-dimensional
activations ($z_\ell \in \mathbb{R}^a$) and $p$-dimensional parameters ($x_\ell
    \in \mathbb{R}^p$):

\begin{align*}
    D_D    & \equiv \begin{bmatrix}
                        \underbrace{I \otimes b_1}_{p \times ap} &        &               \\
                                                                 & \ddots &               \\
                                                                 &        & I \otimes b_L
                    \end{bmatrix}
    ,
    \qquad
    D_M \equiv \begin{bmatrix}
                   \underbrace{I \otimes b_1}_{a \times a^2} &        &               \\
                                                             & \ddots &               \\
                                                             &        & I \otimes b_L
               \end{bmatrix}
    ,                                                                                                \\                                                                                     \\
    P      & \equiv \begin{bmatrix}
                        0 \\ I & 0 \\ &\ddots \\ &I&0
                    \end{bmatrix}
    ,    \qquad
    M       \equiv \begin{bmatrix}
                       I                                                                                    \\
                       \underbrace{-\nabla_z f_2}_{a \times a} & I                                          \\
                                                               & -\nabla_z f_3 & I                          \\
                                                               &               & \ddots & \ddots            \\
                                                               &               &        & -\nabla_z f_L & 1 \\
                   \end{bmatrix}
    ,                                                                                                \\
    D_x    & \equiv  \begin{bmatrix}
                         \underbrace{\nabla_x f_1}_{a\times p} &                       \\
                                                               & \ddots                \\
                                                               &        & \nabla_x f_L
                     \end{bmatrix}
    , \qquad
    D_{xx} \equiv \begin{bmatrix}
                      \underbrace{\nabla_{xx} f_1}_{ap\times p} &        &                 \\
                                                                & \ddots &                 \\
                                                                &        & \nabla_{xx} f_L
                  \end{bmatrix}
    , \qquad
    D_{xz} \equiv
    \begin{bmatrix}
        \underbrace{\nabla_{xz} f_1}_{a^2 \times p} &        &                 \\
                                                    & \ddots &                 \\
                                                    &        & \nabla_{xz} f_L
    \end{bmatrix}
    ,                                                                                                \\
    D_{zx} & \equiv \begin{bmatrix}
                        \underbrace{\nabla_{zx} f_1}_{ap \times a} &        &                 \\
                                                                   & \ddots &                 \\
                                                                   &        & \nabla_{zx} f_L
                    \end{bmatrix}
    ,
    \qquad
    D_{zz}  \equiv \begin{bmatrix}
                       \underbrace{\nabla_{zz} f_1}_{a^2\times a} &        &                 \\
                                                                  & \ddots &                 \\
                                                                  &        & \nabla_{zz} f_L
                   \end{bmatrix}
    .
\end{align*}

\subsection{Mulitplying a vector by the Hessian}

Given a vector $g \in \mathbb{R}^{Lp}$, one the formula above allows us to
compute $H g$ in $O\left(Lap^2 + L a^2p +La^3\right)$ operations without
forming $H$. This cost is dominated by multiplying by the $D_{xx}$, $D_{zx}$,
and $D_{zz}$ matrices.

It's tempting to use this insight to use Krylove methods to solve systems of
the form $H x = b$ without forming $H$. This would require compute $H g$ some
number of times that depends on the condition number of $H$. However, the next
section shows how to compute $H^{-1} b$ with roughly only as many operations as
are needed to comptue $H g$.

\section{The Inverse of the Hessian}

The above shows that the Hessian is a second order matrix polynomial in
$M^{-1}$. While $M$ itself is block-bidiagonal, $M^{-1}$ is dense, so $H$ is
dense. Nevertheless, this polynomial can be lifted into a higher order object
whose inverse is easy to compute:

\begin{align*}
    H & = D_DD_{xx}  + D_DD_{zx} PM^{-1} D_x + D_x^\top M^{-\top}P^\top D_M D_{xz}+D_x^\top M^{-\top}P^\top D_M D_{zz}P M^{-1}D_x \\
      & = \begin{bmatrix}
              M^{-1}D_x \\ I
          \end{bmatrix}
    ^\top
    \begin{bmatrix}
        P^\top D_M D_{zz} P & P^\top D_M D_{xz} \\
        D_D D_{zx} P        & D_D D_{xx}
    \end{bmatrix}
    \begin{bmatrix}
        M^{-1}D_x \\ I
    \end{bmatrix}
    \\
      & = I +  \begin{bmatrix}
                   D_x \\ I
               \end{bmatrix}
    ^\top
    \underbrace{\begin{bmatrix}
                        M^{-\top} & \\ & I
                    \end{bmatrix}
    }_{\hat{M}^{-\top}}
    \underbrace{
        \begin{bmatrix}
            P^\top D_M D_{zz} P & P^\top D_M D_{xz} \\
            D_D D_{zx} P        & D_D D_{xx}-I
        \end{bmatrix}
    }_{\equiv Q}
    \underbrace{\begin{bmatrix}
                        M^{-1} & \\ & I
                    \end{bmatrix}
    }_{\hat{M}^{-1}}
    \begin{bmatrix}
        D_x \\ I
    \end{bmatrix}
    .
\end{align*}

The Woodbury formula gives
\begin{align}
    H^{-1} & = I -
    \begin{bmatrix}
        D_x \\ I
    \end{bmatrix}
    ^\top
    \left( \left(\hat{M}^{-\top} Q \hat{M}^{-1}\right)^{-1} +
    \begin{bmatrix}
        D_x \\ I
    \end{bmatrix}
    \begin{bmatrix}
        D_x \\ I
    \end{bmatrix}
    ^\top
    \right)^{-1}
    \begin{bmatrix}
        D_x \\ I
    \end{bmatrix}
    \nonumber      \\
           & = I -
    \begin{bmatrix}
        D_x \\ I
    \end{bmatrix}
    ^\top
    \left(\underbrace{
        \hat{M} Q^{-1} \hat{M}^\top
        +
        \begin{bmatrix}
            D_x D_x^\top & D_x \\
            D_x^\top     & I
        \end{bmatrix}
    }_{\equiv A}\right)^{-1}
    \begin{bmatrix}
        D_x \\ I
    \end{bmatrix}
    .
\end{align}

The matrix $Q^{-1}$ can be computed explicitly using the partitioned matrix
inverse formula. Define the Schur complement $S = Q_{11} - Q_{12} Q_{22}^{-1}
    Q_{21}$, where $Q_{ij}$ denote the $i,j$th block of $Q$ as defined above. Then
\begin{equation}
    Q^{-1} =
    \begin{bmatrix}
        S^{-1}                      & -S^{-1} Q_{12} Q_{22}^{-1}                                 \\
        - Q_{22}^{-1} Q_{21} S^{-1} & Q_{22}^{-1} + Q_{22}^{-1} Q_{21} S^{-1} Q_{12} Q_{22}^{-1}
    \end{bmatrix}
    .
\end{equation}

The matrices $Q_{11}$, $Q_{12}$, $Q_{21}$, and $Q_{22}$ are all block-diagonal.
$S$ is also block diagonal because $Q_{11}$ and $Q_{12} Q_{22}^{-1} Q_{21}$ are
both block-diagonal. Since all the terms involved in the blocks of $Q^{-1}$ are
block-diagonal, $Q^{-1}$ has the same banded structure as $Q$.

The inverse of $A\equiv \hat{M} Q^{-1} \hat{M}^\top + \begin{bmatrix}
        D_x D_x^\top & D_x \\ D_x^\top & I
    \end{bmatrix}
$ can be applied efficiently. Instead of applying the Woodbury formula again, we compute its $LDL^\top$ decomposition and apply the inverse of that decomposition. The $LDL^\top$ decomposition of $A$ is
\begin{align}
    A      & =
    \begin{bmatrix}
        I & A_{12} A_{22}^{-1} \\ 0 & I
    \end{bmatrix}
    \begin{bmatrix}
        A_{11} - A_{12} A_{22}^{-1} A_{12}^\top & 0      \\
        0                                       & A_{22}
    \end{bmatrix}
    \begin{bmatrix}
        I & A_{12} A_{22}^{-1} \\ 0 & I
    \end{bmatrix}
    ^\top \nonumber                                                      \\
    A_{11} & = M \left[Q^{-1}\right]_{11} M^\top +D_x D_x^\top \nonumber \\
    A_{12} & = M\left[Q^{-1}\right]_{12} + D_x \nonumber                 \\
    A_{22} & = \left[Q^{-1}\right]_{22} + I.
\end{align}
so
\begin{align}
    A^{-1} & = \begin{bmatrix}
                   I & -A_{12} A_{22}^{-1} \\ 0 & I
               \end{bmatrix}
    ^\top
    \begin{bmatrix}
        A_{11} - A_{12} A_{22}^{-1} A_{12}^\top & 0      \\
        0                                       & A_{22}
    \end{bmatrix}
    ^{-1}
    \begin{bmatrix}
        I & -A_{12} A_{22}^{-1} \\ 0 & I
    \end{bmatrix}
\end{align}
Since $A_{11}$ is block tri-diagonal, $A_{12}$ is block-bidiagonal, and $A_{22}$ is block-diagonal, applying $A^{-1}$ to a vector is fast.

\subsection*{Summary: Algorithm to compute $H^{-1} g$}

Given a vector $g \in \mathbb{R}^{Lp}$, compute $H^{-1} g$ as follows:

\begin{enumerate}
    \item
          \textbf{Compute the auxiliary vector:}
          \[
              g'  \in \mathbb{R}^{La + Lp} \equiv \begin{bmatrix}
                  D_x \\ I
              \end{bmatrix}
              g.
          \]
          $D_x$ has $L$ $a\times p$ blocks on its diagonal, so it takes $Lap$ multiplications to compute $v$.

    \item
          \textbf{Form the banded matrix:}
          \[
              A \in \mathbb{R}^{L(a+p) \times L(a+p)} \equiv \hat{M} Q^{-1} \hat{M}^\top +
              \begin{bmatrix}
                  D_x D_x^\top & D_x \\ D_x^\top & I
              \end{bmatrix}
              .
          \]
          To compute $Q^{-1}$, we first compute the blocks of $Q$. These take $La^3$
          multiplications for $Q_{11} \in \mathbb{R}^{La\times La}$, $La^2p$ for
          $Q_{12}\in \mathbb{R}^{L a\times Lp}$ and $Q_{21}\in \mathbb{R}^{Lp \times
                  La}$, and $La^2p$ for $Q_{22}\in \mathbb{R}^{Lp \times Lp}$. Computing $S\in
              \mathbb{R}^{La\times La}$ takes $L p^3$ to compute $Q_{22}^{-1}$, and $2Lap^2$
          to compute the product $Q_{12} Q_{22}^{-1} Q_{21}$. Given these quantities, for
          the blocks of $Q^{-1}$, it takes an additional $La^3$ to compute the upper left
          block, $L(a^2p+ap^2)$ to compute the off-diagonal blocks, and somewhat less
          than that to compute the bottom diagonal block since the matrices involved have
          already been computed. In all, it takes less than $9L\max(a,p)^3$
          multiplications to compute $Q^{-1}$.

          To compute $\hat{M}Q^{-1}\hat{M}^\top$ requires an additional $2La^3$
          operations for a total of $11L\max(a,p)^3$ multiplications.

          Finally, computing and adding the second term requires $La^2p$ multiplications,
          bringing the tally to at most $12 L \max(a,p)^3$ multiplications to compute
          $A$.

    \item
          \textbf{Apply $A^{-1}$ to $g'$:}
          \[
              g'' =
              \begin{bmatrix}
                  I & -A_{12} A_{22}^{-1} \\ 0 & I
              \end{bmatrix}
              ^\top
              \begin{bmatrix}
                  A_{11} - A_{12} A_{22}^{-1} A_{12}^\top & 0      \\
                  0                                       & A_{22}
              \end{bmatrix}
              ^{-1}
              \begin{bmatrix}
                  I & -A_{12} A_{22}^{-1} \\ 0 & I
              \end{bmatrix}
              g'.
          \]
          This computation requires $2L\max(a,p)^3$ multiplications to compute
          $A_{22}^{-1}$ and $\left[A_{11}-A_{12} A_{22}^{-1}A_{12}\right]^{-1}$. The
          remaining operations are matrix multiplications that take at most
          $3L\max(a,p)^2$, which is smaller than $Lp^3$ when $p>3$. This brings the tally
          to at most $15L\max(a,p)^3$ multiplications.

    \item
          \textbf{Compute the final result:}
          \[
              y = g - \begin{bmatrix}
                  D_x \\ I
              \end{bmatrix}
              ^\top g''.
          \]
          These are again matrix-vector multiplications that take at most $L\max(a,p)^2$
          when $p>1$, bringing the tally to at most $16L\max(a,p)^3$.
\end{enumerate}

\bibliographystyle{plain}
\bibliography{hessian}

\end{document}
