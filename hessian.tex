\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{authblk}

\newcommand{\R}{\mathop{\mathbb{R}}}
\newcommand{\K}{\mathcal{K}}

\newtheorem{observation}{Observation}
\newtheorem{example}{Example}

\geometry{margin=1in}

\title{The Hessian of tall-skinny networks is easy to invert}
\author{Ali Rahimi}

\begin{document}

\maketitle
\begin{abstract}
  We show how to solve linear systems that involve the Hessian of a deep net.
  Compared to the naive approach of first computing the Hessian, then solving
  the linear system, which takes storage that's quadratic in the number of
  parameters and cubically many operations, the method scales roughly like
  Pearlmutter's algorithm for computing Hessian-vector products. That means
  Hessian-inverse-vector products can be computed in time and storage that
  scale linearly in the number of parameters.
\end{abstract}

\section{Introduction}

The Hessian of a deep net is the matrix of second-order mixed partial derivatives of its loss with respect to its
parameters. Decades ago, when deep nets had only hundreds or thousands of parameters, the Hessian matrix could be
inverted to implement optimizers that converged much faster than gradient descent \cite{Watrous1987, Barnard1992}.
However, for much larger modern deep nets, using the Hessian has become increasingly less practical: The Hessian of a model
with a billion parameters would have a quintillion entries, which is far larger than can be stored, multiplied,
factorized, or inverted, even in the largest data centers. A common workaround is to approximate the Hessian as a
low-rank matrix \cite{Webb1988, lbfgs-deep-net} or as a diagonal matrix \cite{becker-lecun-89, adagrad, adam}. Such
approximations make it easier to apply the inverse of the Hessian to a vector.

It has long been known that the product of the Hessian with a fixed vector (the so-called Hessian-vector product) can
be computed in time and storage that scales linearly with the number of parameters in the network. This is
dramatically less than the cubic scaling of the naive algorithm that first computes the Hessian matrix and then
multiplies it by the vector. The trick, which is due to Pearlmutter \cite{pearlmutter94}, is to modify the original
network to cause the gradient of the modified network to become the desired Hessian-vector product. Given a way to
compute the Hessian-vector product, one can then compute the Hessian-inverse-vector product by, say Krylov iterations
like Conjugate Gradient. However, the quality of the result would then depend on the conditioning of the Hessian, which
is notoriously poor for deep nets \cite{ying-behrooz-hessian-spectrum}. Unfortunately, there seems to exist no variant
of Pearlmutter's trick to compute the Hessian-inverse-vector products directly.

This article offers a way to compute Hessian-inverse-vector products using roughly as much time as Pearlmutter's
algorithm requires to compute Hessian-vector products. The Hessian of a deep net has a regular structure that makes it
possible to compute Hessian-inverse-vector products exactly and efficiently. Regardless of the specific operations in
each layer, the Hessian of a layerwise deep net is a low-order matrix polynomial that involves the first order and
second order mixed derivatives of each layer, and the inverse of a block-bi-diagonal operator that represents the
backpropoagation algorithm. Exploiting this structure offers a a direct way to compute Hessian-vector products using
exactly the same operations as Pearlmutter's algorithm (see Appendix \ref{sec:pearlmutter}). It also leads to a way to
compute Hessian-inverse-vector product that does not require forming or storing the full Hessian, and without incurring
quadratically (let alone cubically) many FLOPs. For an $L$-layer deep net where each layer has $p$ parameters and
produces $a$ activations, naively storing the Hessian would require $O(L^2p^2)$ memory, and multiplying it by a vector
or solving a linear system would require $O(L^2p^2 + L\max(a,p)^3)$ and $O(L^3p^3)$ operations, respectively. In
contrast, we will show how to perform these operations using only $O(L \max(a, p)^3)$ computations. The dependence on
the parameters on the number of activations and parameters in each layer is still cubic, but the dependence on the
number of layers is only linear. This makes operating on the Hessian of tall and skinny networks more efficient than
the Hessian of short and fat networks.

\section{Overview}

Our objective is to efficiently solve linear systems of the form $H x = g$, where $H$ is the Hessian of a deep neural
network, without forming $H$ explicitly. To do this, we employ the following strategy:
\begin{enumerate}
  \item
        Write down the gradient of the deep net in matrix form, as a bi-diagonal system of linear
        equations. Solving this system uses back-substitution, which in this case
        coincides exactly with the computations carried out by backpropagation.
  \item
        Differentiate this matrix form to obtain the second-order derivatives. Write the result in matrix form,
        which involves a second-order polynomial in the inverse of the aforementioned bi-diagonal matrix.
  \item
        Lift this polynomial into a higher-dimensional matrix form, and use the Woodbury formula to invert the resulting
        matrix.
  \item
        Derive a fast algorithm to apply the inverse of such polynomials.
\end{enumerate}

\section{Notation}

We write a deep net as a pipeline of functions $\ell = 1, \ldots, L$,
\begin{align}
  z_1    & = f_1(z_0; x_1) \nonumber              \\
         & \ldots \nonumber                       \\
  z_\ell & = f_\ell(z_{\ell-1}; x_\ell) \nonumber \\
         & \ldots \nonumber                       \\
  z_L    & = f_L(z_{L-1}; x_L)
\end{align}

The vectors $x_1,\ldots, x_L$ are the parameters of the pipeline. The vectors $z_1,\ldots, z_L$ are its intermediate
activations, and $z_0$ is the input to the pipeline. The last layer $f_L$ computes the final activations and their
training loss, so the scalar $z_L$ is the loss of the model on the input $z_0$. To make this loss's dependence on $z_0$
and the parameters explicit, we sometimes write it as $z_L(z_0;x)$. This formalization deviates slightly from the
traditional deep net formalism in two ways: First, the training labels are subsumed in $z_0$, and are propagated
through the layers until they're used in the loss. Second, the last layer fuses the loss (which has no parameters) and
the last layer (which does).

We'll assume the first and partial derivatives of each layer with respect to its
parameters and its inputs exist. This poses some complications with ReLU
activations and other non-differentiable operations in modern networks. Notably,
for the Hessian to be symmetric, it must be differentiable everywhere.  We'll
largely ignore this complication and assume that differentiable approximations to these
operations are used.

At the end of each section, we'll count the number of floating point operations required to compute various
expressions. While the derivations do not impose any restrictions on the shape of the layers, for the purpose of this
accounting, we'll assume all but the last $L$ layers have $a$-dimensional activations ($z_\ell \in \R^a$) and
$p$-dimensional parameters ($x_\ell \in \R^p$).

\section{Backpropagation, the matrix way}

We would like to fit the vector of parameters $x = (x_1, \ldots, x_L)$ given a training dataset, which we represent by
a stochastic input $z_0$ to the pipeline. Training the model proceeds by gradient descent steps along the stochastic
gradient $\partial z_L(z_0;x) / \partial x$. The components of this direction can be computed by the chain rule with a
backward recursion:
\begin{align}
  \label{eq:backprop}
  \frac{\partial z_L}{\partial x_\ell} & = \underbrace{\frac{\partial z_L}{\partial z_{\ell}}}_{b_\ell} \underbrace{\frac{\partial z_\ell}{\partial x_\ell}}_{\nabla_x f_\ell}                \\
  \frac{\partial z_L}{\partial z_\ell} & = \underbrace{\frac{\partial z_L}{\partial z_{\ell+1}}}_{b_{\ell+1}} \underbrace{\frac{\partial z_{\ell+1}}{\partial z_\ell}}_{\nabla_z f_{\ell+1}}.
\end{align}
The identification $b_\ell \equiv \frac{\partial z_L}{\partial z_\ell}$, $\nabla_x f_\ell \equiv \frac{\partial z_\ell}{\partial x_\ell}$, and $\nabla_z f_\ell \equiv \frac{\partial z_\ell}{\partial z_{\ell-1}}$ turns this recurrence into
\begin{align}
  \frac{\partial z_L}{\partial x_\ell} & = b_\ell \cdot \nabla_x f_\ell          \\
  b_\ell                               & = b_{\ell+1} \cdot \nabla_z f_{\ell+1},
\end{align}
with the base case $b_L = 1$, a scalar.  These two equations can be written in vector form as
\begin{equation}
  \frac{\partial z_L}{\partial x} =
  \begin{bmatrix}
    \frac{\partial z_L}{\partial x_1} & \cdots & \frac{\partial z_L}{\partial x_L}
  \end{bmatrix}
  =
  \underbrace{
    \begin{bmatrix}
      b_1 & \cdots & b_L
    \end{bmatrix}
  }_{\equiv b}
  \underbrace{
    \begin{bmatrix}
      \nabla_x f_1 &                       \\
                   & \ddots                \\
                   &        & \nabla_x f_L
    \end{bmatrix}
  }_{\equiv D_x},
\end{equation}
and
\begin{equation}
  \begin{bmatrix}
    b_1 & b_2 & b_3 & \cdots & b_{L-1} & b_L
  \end{bmatrix}
  \underbrace{
    \begin{bmatrix}
      I                                                          \\
      -\nabla_z f_2 & I                                          \\
                    & -\nabla_z f_3 & I                          \\
                    &               & \ddots & \ddots            \\
                    &               &        & -\nabla_z f_L & 1 \\
    \end{bmatrix}
  }_{\equiv M}
  =
  \underbrace{
    \begin{bmatrix}
      0 & \cdots & 1
    \end{bmatrix}
  }_{\equiv e_L}.
\end{equation}

Solving for $b$ and substituting back gives
\begin{equation}
  \frac{\partial z_L}{\partial x} =  e_L M^{-1} D_x.
\end{equation}

The matrix $M$ is block bi-diagonal. Its diagonal entries are identity matrices, and its off-diagonal matrices are the
gradient of the intermediate activations with respect to the layer's parameters. The matrix $D_x$ is block diagonal,
with the block as the derivative of each layer's activations with respect to its inputs. $M$ is invertible because the
spectrum of a triangular matrix can be read off its diagonal, which in this case is all ones.

\section{The Hessian}

The gradient we computed above is the unique vector $v$ such that $d z_L \equiv z_L(x+dx) - z_L(dx) \to v(x) \cdot dx$
as $dx\to 0$. In this section, we compute the Hessian $H$ of $z_L$ with respect to the parameters. This is the unique
matrix $H(x)$ such that $dv^\top \equiv v^\top(x+dx) - v^\top(x) \to H(x)\; dx$ as $dx \to 0$. We use the facts that
$dM^{-1} = -M^{-1} (dM) M^{-1}$ and $b=e_L M^{-1}$ to write
\begin{align}
  dv & = d(e_L M^{-1} D_x) \nonumber                               \\
     & = e_L M^{-1} (dD_x)+ e_L \left(dM^{-1}\right) D_x \nonumber \\
     & = b \cdot dD_x  - e_L M^{-1} (dM) M^{-1} D_x \nonumber      \\
     & = b \cdot dD_x  - b \cdot (dM) M^{-1} D_x
\end{align}

We compute each of these terms separately. As part of this agenda, we will rely on the gradient of tensor-valued
functions $g:\R^d \to \R^{o_1 \times \cdots \times o_k}$. Define this gradient $\nabla_x g(x)\in \R^{(o_1 \cdots o_k)
    \times d}$ as the unique matrix-valued function that satisfies

\begin{equation}
  \mathrm{vec} \left(g(x+dx) - g(x)\right) \to \nabla_x g(x)
  \cdot dx
\end{equation}

as $dx \to 0$. This convention readily implies the Hessian of a vector-valued function: If $g:\R^d \to \R^o$, then
$\nabla_{xx} g(x) \in \R^{o\times d^2}$ is the unique matrix such that $\mathrm{vec} \left(\nabla_x g(x+dx) - \nabla_x
  g(x)\right) \to \nabla_{xx} g(x) \; dx$. This convention also readily accommodates the chain rule. For example, the
gradient of $h(x) \equiv f(g(x))$ for matrix-valued $f$ and $g$ can be written as $\nabla f \nabla g$ as expected. It
also implies partial derivatives like $\nabla_{yz} g$ for $g:\R^{|x|} \to \R^{|g|}$. If $y\in \R^{|y|}$ and $z\in
  \R^{|z|}$ are restrictions of $x \in \R^{|x|}$ to some $|y|$ and $|z|$-dimensional subsets, then $\nabla_z g(x) \in
  \R^{|g| \times |z|}$, and $\nabla_{yz} g(x) = \nabla_y \nabla_z g(x) \in \R^{|g||z| \times |y|}$. See Chapter 6 of
Magnus \& Neudecker \cite{magnus-neudecker} for a deeper treatment of higher order derivatives of vector-valued
functions.

\subsection{The term involving $dD_x$}

The matrix $D_x$ is block-diagonal with its $\ell$th diagonal block containing the matrix $D_\ell \equiv \nabla_x
  f_\ell$. Using the facts that $\mathrm{vec}(ABC) = \left(C^\top \otimes A \right) \mathrm{vec}(B)$, and $(A\otimes
  B)^\top = A^\top \otimes B^\top$, we get
\begin{align}
  b\cdot (dD_x) & =
  \begin{bmatrix}
    b_1 & \cdots & b_L
  \end{bmatrix}
  \begin{bmatrix}
    dD_1 &        &      \\
         & \ddots &      \\
         &        & dD_L
  \end{bmatrix}
  \nonumber         \\
                & =
  \begin{bmatrix}
    b_1 \cdot dD_1 & \cdots & b_L \cdot dD_L
  \end{bmatrix}
  \nonumber         \\
                & =
  \begin{bmatrix}
    \mathrm{vec}\left(dD_1\right)^\top \left(I \otimes b_1^\top\right) &
    \cdots                                                             &
    \mathrm{vec}\left(dD_L\right)^\top \left(I \otimes b_L^\top\right)
  \end{bmatrix}
  \nonumber         \\
                & =
  \begin{bmatrix}
    \mathrm{vec}\left(dD_1\right) \\
    \vdots                        \\
    \mathrm{vec}\left(dD_L\right)
  \end{bmatrix}
  ^\top
  \begin{bmatrix}
    I \otimes b_1^\top &        &                    \\
                       & \ddots &                    \\
                       &        & I \otimes b_L^\top
  \end{bmatrix}
\end{align}

Observe that $\mathrm{vec}\left(dD_\ell\right) = d\,\mathrm{vec} \nabla_x f_\ell(z_{\ell-1}; x_\ell)$ varies with $dx$
through both its arguments $x_\ell$ and $z_{\ell-1}$. Using mixed partials of vector-valued functions described above,
we get
\begin{equation}
  \mathrm{vec} \left(dD_\ell\right) = d\,\mathrm{vec}\left(\nabla_x f_\ell\right) = \left(\nabla_{xx} f_\ell\right)\; dx_\ell  + \left(\nabla_{zx} f_\ell\right)\; dz_{\ell-1}.
\end{equation}

Stacking these equations gives
\begin{align}
  \begin{bmatrix}
    \mathrm{vec}\left(dD_1\right) \\
    \vdots                        \\
    \mathrm{vec}\left(dD_L\right)
  \end{bmatrix}
   & =
  \begin{bmatrix}
    \nabla_{xx} f_1 &        &                 \\
                    & \ddots &                 \\
                    &        & \nabla_{xx} f_L
  \end{bmatrix}
  dx
  +
  \begin{bmatrix}
    \nabla_{zx} f_1 &        &                 \\
                    & \ddots &                 \\
                    &        & \nabla_{zx} f_L
  \end{bmatrix}
  \begin{bmatrix}
    dz_0 \\ \vdots \\ dz_{L-1}
  \end{bmatrix}
  .
\end{align}

Each vector $dz_\ell$ in turn varies with $dx$ via $dz_\ell = (\nabla_x f_\ell) dx_\ell + (\nabla_z f_\ell)
  dz_{\ell-1}$, with the base case $dz_0 = 0$, since the input $z_0$ does not vary with $dx$. Stacking up this recurrence
gives
\begin{align}
  \begin{bmatrix}
    I             &   &                   \\
    -\nabla_z f_2 & I &                   \\
                  &   & \ddots            \\
                  &   & -\nabla_z f_L & 1 \\
  \end{bmatrix}
  \begin{bmatrix}
    dz_1 \\ \vdots \\ dz_{L-1}  \\ dz_L
  \end{bmatrix}
  =
  \begin{bmatrix}
    \nabla_x f_1 &        &              \\
                 & \ddots &              \\
                 &        & \nabla_x f_L
  \end{bmatrix}
  dx.
\end{align}
We can solve for the vector $
  \begin{bmatrix}
    dz_1 \\ \vdots \\ dz_L
  \end{bmatrix}
  = M^{-1} D_x dx$ and use the downshifting matrix
\begin{equation}
  \label{eq:P}
  P \equiv
  \begin{bmatrix}
    0 \\ I & 0 \\ &\ddots \\ &I&0
  \end{bmatrix}
\end{equation}
to plug back the vector $
  \begin{bmatrix}
    dz_0 \\ \vdots \\ dz_{L-1}
  \end{bmatrix}
  =PM^{-1}D_x dx$:
\begin{align}
  \begin{bmatrix}
    \mathrm{vec}\left(dD_1\right) \\
    \vdots                        \\
    \mathrm{vec}\left(dD_L\right)
  \end{bmatrix}
  =
  \left(
  \begin{bmatrix}
    \nabla_{xx} f_1 &        &                 \\
                    & \ddots &                 \\
                    &        & \nabla_{xx} f_L
  \end{bmatrix}
  +
  \begin{bmatrix}
    \nabla_{zx} f_1 \\ &\ddots& \\ &&\nabla_{zx} f_L
  \end{bmatrix}
  P M^{-1} D_x
  \right)dx.
\end{align}

\subsection{The term involving $dM$}

The matrix $dM$ is lower-block-diagonal with $dM_2,\ldots, dM_L$, and $dM_\ell \equiv d \nabla_z f_\ell$. Similar to
the above, we can write
\begin{align}
  b & \cdot (dM) M^{-1} D_x =
  \begin{bmatrix}
    b_1 & \cdots & b_{L-1} & b_L
  \end{bmatrix}
  \begin{bmatrix}
    0                  \\
    -dM_2 & 0          \\
          & \ddots     \\
          & -dM_L  & 0 \\
  \end{bmatrix}
  M^{-1} D_x                  \\
    & = -
  \begin{bmatrix}
    b_2 \cdot dM_2 & \cdots & b_L \cdot dM_L & 0
  \end{bmatrix}
  M^{-1} D_x                  \\
    & = -
  \begin{bmatrix}
    \mathrm{vec} \left(dM_2\right)^\top \left(I \otimes b_2^\top\right) &
    \cdots                                                              &
    \mathrm{vec} \left(dM_L\right)^\top \left(I \otimes b_L^\top\right) &
    0
  \end{bmatrix}
  M^{-1} D_x                  \\
    & =
  -
  \begin{bmatrix}
    \mathrm{vec} \left(dM_1\right) \\
    \vdots                         \\
    \mathrm{vec} \left(dM_L\right)
  \end{bmatrix}
  ^\top
  \begin{bmatrix}
    0                                               \\
    I \otimes b_2^\top & 0                          \\
                       &   & \ddots                 \\
                       &   & I \otimes b_L^\top & 0
  \end{bmatrix}
  M^{-1} D_x                  \\
    & = -
  \begin{bmatrix}
    \mathrm{vec} \left(dM_1\right) \\
    \vdots                         \\
    \mathrm{vec} \left(dM_L\right)
  \end{bmatrix}
  ^\top
  \begin{bmatrix}
    I \otimes b_1^\top \\ &\ddots  \\ && I \otimes b_L^\top
  \end{bmatrix}
  PM^{-1} D_x.
\end{align}

Each matrix $dM_\ell = d \nabla_z f_\ell(z_{\ell-1}; x_\ell)$ varies with $dx$ through both $x_\ell$ and $z_{\ell -1}$
as $d\,\mathrm{vec} \left(M_\ell\right) = \left(\nabla_{xz} f_\ell\right) dx_\ell + \left(\nabla_{zz} f_\ell\right)
  dz_{\ell-1}$. Following the steps of the previous section gives
\begin{align}
  \begin{bmatrix}
    \mathrm{vec}\left(dM_1\right) \\
    \vdots                        \\
    \mathrm{vec}\left(dM_L\right)
  \end{bmatrix}
  =
  \left(
  \begin{bmatrix}
    \nabla_{xz} f_1 &        &                 \\
                    & \ddots &                 \\
                    &        & \nabla_{xz} f_L
  \end{bmatrix}
  +
  \begin{bmatrix}
    \nabla_{zz} f_1 \\ &\ddots& \\ && \nabla_{zz} f_L
  \end{bmatrix}
  P M^{-1} D_x
  \right)dx.
\end{align}

\subsection{Putting it all together}

We have just shown that the Hessian of the deep net has the form
\begin{align}
  H \equiv \frac{\partial^2 z_L}{\partial x^2}
   & = D_D \left(D_{xx} + D_{zx} PM^{-1} D_x\right) + D_x^\top M^{-T}P^\top D_M \left(D_{xz}+D_{zz}P M^{-1}D_x\right)           \\
  \label{eq:hessian}
   & = D_DD_{xx}  + D_DD_{zx} PM^{-1} D_x + D_x^\top M^{-\top}P^\top D_M D_{xz}+D_x^\top M^{-\top}P^\top D_M D_{zz}P M^{-1}D_x.
\end{align}

The various matrices involved are recapitulated below:

\begin{align*}
  D_D    & \equiv
  \begin{bmatrix}
    \underbrace{I \otimes b_1}_{p \times ap} &        &               \\
                                             & \ddots &               \\
                                             &        & I \otimes b_L
  \end{bmatrix}
  ,
  \qquad
  D_M \equiv
  \begin{bmatrix}
    \underbrace{I \otimes b_1}_{a \times a^2} &        &               \\
                                              & \ddots &               \\
                                              &        & I \otimes b_L
  \end{bmatrix}
  ,               \\                                                                                     \\
  P      & \equiv
  \begin{bmatrix}
    0 \\ I & 0 \\ &\ddots \\ &I&0
  \end{bmatrix}
  ,    \qquad
  M       \equiv
  \begin{bmatrix}
    I                                                                                    \\
    \underbrace{-\nabla_z f_2}_{a \times a} & I                                          \\
                                            & -\nabla_z f_3 & I                          \\
                                            &               & \ddots & \ddots            \\
                                            &               &        & -\nabla_z f_L & 1 \\
  \end{bmatrix}
  ,               \\
  D_x    & \equiv
  \begin{bmatrix}
    \underbrace{\nabla_x f_1}_{a\times p} &                       \\
                                          & \ddots                \\
                                          &        & \nabla_x f_L
  \end{bmatrix}
  , \qquad
  D_{xx} \equiv
  \begin{bmatrix}
    \underbrace{\nabla_{xx} f_1}_{ap\times p} &        &                 \\
                                              & \ddots &                 \\
                                              &        & \nabla_{xx} f_L
  \end{bmatrix}
  , \qquad
  D_{xz} \equiv
  \begin{bmatrix}
    \underbrace{\nabla_{xz} f_1}_{a^2 \times p} &        &                 \\
                                                & \ddots &                 \\
                                                &        & \nabla_{xz} f_L
  \end{bmatrix}
  ,               \\
  D_{zx} & \equiv
  \begin{bmatrix}
    \underbrace{\nabla_{zx} f_1}_{ap \times a} &        &                 \\
                                               & \ddots &                 \\
                                               &        & \nabla_{zx} f_L
  \end{bmatrix}
  ,
  \qquad
  D_{zz}  \equiv
  \begin{bmatrix}
    \underbrace{\nabla_{zz} f_1}_{a^2\times a} &        &                 \\
                                               & \ddots &                 \\
                                               &        & \nabla_{zz} f_L
  \end{bmatrix}
  .
\end{align*}
\subsection{Multiplying a vector by the Hessian}

Given a vector $g \in \R^{Lp}$, the formula above allows us to compute $H g$ in
$O\left(Lap^2 + L a^2p +La^3\right)$ operations without forming $H$. This cost
is dominated by multiplying by the $D_{xx}$, $D_{zx}$, and $D_{zz}$ matrices.
Appendix \ref{sec:pearlmutter} shows that these operations are exactly the
operations involved in Pearlmutter's trick to compute the Hessian-vector
product.

It's tempting to use this insight to use Krylov methods to solve systems of the form $H x = b$ without forming $H$.
This would require computing $H g$ some number of times that depends on the condition number of $H$. However, the next
section shows how to compute $H^{-1} b$ with roughly only as many operations as are needed to compute $H g$.

\section{The inverse of the Hessian}

The above shows that the Hessian is a second order matrix polynomial in
$M^{-1}$. While $M$ itself is block-bidiagonal, $M^{-1}$ is dense, so $H$ is
dense. Nevertheless, this polynomial can be lifted into a higher order object
whose inverse is easy to compute:

\[
  H  = D_D D_{xx}  + D_D D_{zx} P M^{-1} D_x + D_x^\top M^{-\top} P^\top D_M D_{xz} + D_x^\top M^{-\top} P^\top D_M D_{zz} P M^{-1} D_x
\]
We wish to solve the system $H x = g$ for $x$. We can convert this dense system
involving inverses into a larger, sparse system by introducing auxiliary
variables. Define
\begin{equation}
  y \equiv M^{-1} D_x x,
\end{equation}
which implies $M y - D_x x = 0$.  Define a second auxiliary variable
\begin{equation}
  z \equiv M^{-\top} \left( P^\top D_M D_{xz} x + P^\top D_M D_{zz} P y \right),
\end{equation}
which implies $M^\top z - P^\top D_M D_{xz} x - P^\top D_M D_{zz} P y = 0$.
The original equation $H x = g$ becomes
\begin{equation}
  D_D D_{xx} x + D_D D_{zx} P y + D_x^\top z = g.
\end{equation}
Collecting these three linear equations gives us a unified system

\begin{equation}
  \underbrace{
    \begin{bmatrix}
      D_D D_{xx}         & D_D D_{zx} P         & D_x^\top \\
      -D_x               & M                    & 0        \\
      -P^\top D_M D_{xz} & -P^\top D_M D_{zz} P & M^\top
    \end{bmatrix}
  }_{\K}
  \begin{bmatrix}
    x \\ y \\ z
  \end{bmatrix}
  =
  \begin{bmatrix}
    g \\ 0 \\ 0
  \end{bmatrix}.
\end{equation}

Finding $x = H^{-1}g$ is equivalent to solving this block-linear system and
reporting the resulting $x$. The benefit of doing this is that
this system can be pivoted into a block-tri-diagonal system, which can be solved more
efficiently than Gaussian elimination on $\K$ or $H$.

The pivoting we'll apply reorders the blocks of the variables $x,y,z$ from
$x_1,\ldots,x_L, y_1,\ldots, y_L, z_1,\ldots, z_L$ to
$x_1,y_1,z_1,\dots,x_L,y_L,z_L$.  To see why such a pivoting results in a
simpler to solve system, denote the $ij$th block of $\K$ by $\K_{ij}$ and the
$uv$th sub-block of this block by $\K_{ij,uv}$.  Let $x$ denote a vector
conformant with $\K$, with $x_j$ denoting the column vector that multiplies each
block $\K_{\cdot j}$ and $x_{jv}$ denoting the $v$th sub-block of $x_j$, which
multiplies $\K_{\cdot j,\cdot v}$.  We say that $x$ is $j$-major because as we
traverse the entries of $x$ from top to bottom, the index $j$ increments more
slowly than the $v$ index.

Denote by $\Pi$ the permutation that transposes the
blocks with the sub-blocks.  Applying this permutation matrix $\Pi$ to $x$ reorders
it from $j$-major to $d$-major. Applying it to the rows and columns
columns of $\K$ results in a block matrix $\K' \equiv \Pi
  \K \Pi^\top$ that satisfies $\K_{ij,uv} = \K'_{uv,ij}$.

\begin{observation}
  The permutation $\Pi$ is involutory, meaning $\Pi^{-1} =\Pi$.
\end{observation}

This implies that  to solve a system $\K x = b$, we can instead solve $\K' x' =
  \Pi b$ for $x'$, then report $x = \Pi x'$.

This permutation reduces the bandwidth of certain block matrices:

\begin{observation}
  Suppose each block of $\K$ is banded with bandwidth $w$. Then
  $\K'$  is block-banded with bandwidth $w$.
\end{observation}

Notably, even when $\K$ has very large bandwidth, $\K'$ will have bandwidth no
greater than that of the blocks of $\K$.  Section \ref{sec:pivoting-examples}
illustrates this point with some examples.

The blocks in $\K$ are either block-diagonal, block-upper-bi-diagonal, or
block-lower-bi-diagonal, so $\K'$ is at most block-tri-diagonal:
\[
  \K' = \begin{bmatrix}
    \begin{bmatrix}
      K_{11,11} & K_{12,11} & K_{13,11} \\
      K_{21,11} & K_{22,11} & K_{23,11} \\
      K_{31,11} & K_{32,11} & K_{33,11} \\
    \end{bmatrix} & \begin{bmatrix}
                      K_{11,12} & K_{12,12} & K_{13,12} \\
                      K_{21,12} & K_{22,12} & K_{23,12} \\
                      K_{31,12} & K_{32,12} & K_{33,12} \\
                    \end{bmatrix}                                                                           \\
    \begin{bmatrix}
      K_{11,21} & K_{12,21} & K_{13,21} \\
      K_{21,21} & K_{22,21} & K_{23,21} \\
      K_{31,21} & K_{32,21} & K_{33,21} \\
    \end{bmatrix} & \begin{bmatrix}
                      K_{11,22} & K_{12,22} & K_{13,22} \\
                      K_{21,22} & K_{22,22} & K_{23,22} \\
                      K_{31,22} & K_{32,22} & K_{33,22} \\
                    \end{bmatrix} &
    \begin{bmatrix}
      K_{11,23} & K_{12,23} & K_{13,23} \\
      K_{21,23} & K_{22,23} & K_{23,23} \\
      K_{31,23} & K_{32,23} & K_{33,23} \\
    \end{bmatrix}                                                                                           \\
                                         & \begin{bmatrix}
                                             K_{11,32} & K_{12,32} & K_{13,32} \\
                                             K_{21,32} & K_{22,32} & K_{23,32} \\
                                             K_{31,32} & K_{32,32} & K_{33,32} \\
                                           \end{bmatrix} & \begin{bmatrix}
                                                             K_{11,33} & K_{12,33} & K_{13,33} \\
                                                             K_{21,33} & K_{22,33} & K_{23,33} \\
                                                             K_{31,33} & K_{32,33} & K_{33,33} \\
                                                           \end{bmatrix} & \begin{bmatrix}
                                                                             K_{11,34} & K_{12,34} & K_{13,34} \\
                                                                             K_{21,34} & K_{22,34} & K_{23,34} \\
                                                                             K_{31,34} & K_{32,34} & K_{33,34} \\
                                                                           \end{bmatrix}
    \\
                                         & \ddots                               & \ddots                               & \ddots \\
  \end{bmatrix}
\]

\bibliographystyle{plain}
\bibliography{hessian}

\appendix
\section{Equation (\ref{eq:hessian}) is Pearlmutter's Hessian-vector multiplication algorithm}
\label{sec:pearlmutter}

We showed that Equation (\ref{eq:hessian}) makes it possible to compute Hessian-vector product $H v$ in time linear in
$L$. These operations are equivalent to Pearlmutter's \cite{pearlmutter94} algorithm, a framework to compute
Hessian-vector products in networks with arbitrary topologies. This section specializes Pearlmutter's machinery to the
pipeline topology, and shows that the operations it produces coincide exactly with those of Equation
(\ref{eq:hessian}).

Consider a set of vectors $v_1,\ldots,v_L$ that match the dimensions of the parameter vectors $x_1,\ldots,x_L$. Just as
$z_L(x_1,\ldots,x_L)$ denotes the loss under the parameters $w$, we'll consider the perturbed loss $z_L(x_1+\alpha v_1,
  \ldots, x_L+\alpha v_L)$ with a scalar $\alpha$. By the chain rule,
\begin{equation}
  \frac{\partial}{\partial \alpha} z_L(x_1+\alpha_1 v_1, \ldots, x_L+\alpha_L v_L) \bigg|_{\alpha=0} =
  \nabla_x z_L(x_1, \ldots, x_L) \cdot v.
\end{equation}
Applying $\nabla_x$ to both sides gives
\begin{equation}
  \nabla_x \frac{\partial}{\partial \alpha} z_L(x_1+\alpha_1 v_1, \ldots, x_L+\alpha_L v_L) \bigg|_{\alpha=0} =
  \nabla_x^2 z_L(x_1, \ldots, x_L) \cdot v.
\end{equation}
In other words, to compute the Hessian-vector product $\nabla_x^2 z_L \cdot v$,
it suffices to compute the gradient of $\frac{\partial z_L}{\partial \alpha} $
with respect to $x$.  We can do this by applying standard backpropoagation
to $\frac{\partial z_L}{\partial \alpha}$. At each stage $\ell$ during its
backward pass, backpropagation produces $\frac{\partial}{\partial x_\ell}
  \frac{\partial z_L}{\partial \alpha} = \nabla_{x_\ell, x} z_L \cdot v$, yielding
a block of rows in in $\nabla_x^2 z_L \cdot v$.

To see that this generates the same operations as applying Equation (\ref{eq:hessian}) to $v$, we'll write the backprop
operations from Equation (\ref{eq:backprop}) against $\frac{\partial z_L}{\partial \alpha}$ explicitly. We'll use again
the fact that $z_\ell$ depends on $\alpha$ through both $z_{\ell-1}$ and $x_\ell + \alpha v_\ell$ to massage the
backward recursion of for $\frac{\partial z_L}{\partial \alpha}$ into a format that matches Equation
(\ref{eq:backprop}):
\begin{align}
  b'_\ell & \equiv \frac{\partial}{\partial z_\ell} \frac{\partial z_L}{\partial \alpha}
  = \frac{\partial}{\partial \alpha} \frac{\partial z_L}{\partial z_\ell}
  = \frac{\partial}{\partial \alpha} b_\ell
  = \frac{\partial}{\partial \alpha} \left[b_{\ell+1} \cdot \nabla_z f_{\ell+1}\right]                                                                                                                                                 \\
          & = b'_{\ell+1} \cdot \nabla_z f_{\ell+1} + \left[\left(I \otimes b_{\ell+1}\right) \frac{\partial}{\partial \alpha} \text{vec}\left(\nabla_z f_{\ell+1}\right) \right]^\top                                                 \\
          & = b'_{\ell+1} \cdot \nabla_z f_{\ell+1} + \left[\left(I \otimes b_{\ell+1}\right) \left(\nabla_{zz} f_{\ell+1} \cdot \frac{\partial z_\ell}{\partial \alpha} + \nabla_{xz} f_{\ell+1} \cdot v_{\ell+1}\right)\right]^\top.
\end{align}
During the backward pass, from $b_\ell$ and $b'_\ell$, we compute
\begin{align}
  \frac{\partial }{\partial x_\ell} \frac{\partial z_L}{\partial \alpha}
   & =\left(\nabla_{x_\ell, x} z_L\right) \cdot v
  = \frac{\partial}{\partial \alpha} \frac{\partial z_L}{\partial x_\ell}
  = \frac{\partial}{\partial \alpha} \left[\frac{\partial z_L}{\partial z_{\ell}} \frac{\partial z_\ell}{\partial x_\ell}\right]
  = \frac{\partial}{\partial \alpha} \left[b_\ell \cdot \nabla_x f_\ell\right]                                                                                                                             \\
   & = b'_\ell \cdot \nabla_x f_\ell + \left[\left(I\otimes b_\ell\right) \frac{\partial}{\partial \alpha} \text{vec}\left(\nabla_x f_\ell\right)\right]^\top                                              \\
   & = b'_\ell \cdot \nabla_x f_\ell + \left[\left(I\otimes b_\ell\right) \left(\nabla_{zx} f_\ell \cdot \frac{\partial z_{\ell-1}}{\partial \alpha} + \nabla_{xx} f_\ell \cdot v_\ell\right)\right]^\top.
\end{align}
Stacking these backward equations horizontally with $b'\equiv\left[
    \begin{smallmatrix}
      b'_1 & \cdots & b'_L
    \end{smallmatrix}
    \right]$,
$g^\alpha_\ell \equiv \frac{\partial z_\ell}{\partial \alpha}$ and $g^\alpha\equiv\left[
    \begin{smallmatrix}
      g^\alpha_1 \\ \vdots \\ g^\alpha_L
    \end{smallmatrix}
    \right]$, then transposing, gives
\begin{align}
  \label{eq:pearlmutter-2}
  M^\top \left(b'\right)^\top & = P D_M  \left(D_{zz} g^\alpha + D_{xz} v\right) \nonumber                       \\
  \nabla_x^2 z_L \cdot v      & = D_x^\top \left(b'\right)^\top + D_D \left(D_{zx} P g^\alpha + D_{xx} v\right).
\end{align}

$g^\alpha_\ell$ can be computed during the forward pass via
\begin{equation}
  g^\alpha_\ell \equiv \frac{\partial z_\ell}{\partial \alpha}
  = \nabla_z f_\ell \cdot \frac{\partial z_{\ell-1}}{\partial \alpha} + \nabla_x f_\ell \cdot v_\ell
  = \nabla_z f_\ell \cdot g^\alpha_{\ell-1} + \nabla_x f_\ell \cdot v_\ell,
\end{equation}
which when stacked up, gives $M g^\alpha = D_x v$.  Plugging $g^\alpha$ back
into Equation (\ref{eq:pearlmutter-2}) and solving for $b'$ gives
\begin{equation}
  \nabla_x^2 z_L \cdot v = D_x^\top M^{-\top} P D_M  \left(D_{zz} M^{-1} D_x v + D_{xz} v\right) +  D_D \left(D_{zx} P M^{-1} D_x v + D_{xx} v\right).
\end{equation}
This coincides with Equation (\ref{eq:hessian}), showing that the two algorithms are equivalent.

\section{Examples of Pivoting Matrices to Block-Banded Form}
\label{sec:pivoting-examples}

\begin{example}
  Suppose each block $D_{ij}$ of $\K$ is diagonal. Then the full $4\times4$ matrix is
  \[
    \K = \begin{bmatrix}
      D_{11} & D_{12} \\
      D_{21} & D_{22}
    \end{bmatrix} =
    \begin{bmatrix}
      a & 0 & c & 0 \\
      0 & b & 0 & d \\
      e & 0 & g & 0 \\
      0 & f & 0 & h
    \end{bmatrix},
  \]
  where
  \[
    D_{11} = \begin{bmatrix} a & 0 \\ 0 & b \end{bmatrix}, \quad
    D_{12} = \begin{bmatrix} c & 0 \\ 0 & d \end{bmatrix}, \quad
    D_{21} = \begin{bmatrix} e & 0 \\ 0 & f \end{bmatrix}, \quad
    D_{22} = \begin{bmatrix} g & 0 \\ 0 & h \end{bmatrix}.
  \]

  The vector $x = \begin{bmatrix} x_1^1 \\ x_1^2 \\ x_2^1 \\ x_2^2
    \end{bmatrix}$ (in $j$-major order) is reordered by $\Pi$ into
  $\begin{bmatrix} x_1^1 \\ x_2^1 \\ x_1^2 \\ x_2^2 \end{bmatrix}$ ($v$-major order).
  Then
  \[\Pi \K \Pi =
    \begin{bmatrix}
      \hspace{0.3em}
      \begin{smallmatrix}
        \rule{0pt}{2.2ex}
        D_{11, 11} & D_{12, 11} \\
        D_{21, 11} & D_{21, 11}
        \rule[-1.2ex]{0pt}{0pt}
      \end{smallmatrix}
      \hspace{0.8em} &
      \hspace{0.3em}
      \begin{smallmatrix}
        \rule{0pt}{2.2ex}
        D_{11, 12} & D_{12, 12} \\
        D_{21, 12} & D_{21, 12}
        \rule[-1.2ex]{0pt}{0pt}
      \end{smallmatrix}
      \hspace{0.3em}   \\
      \hspace{0.3em}
      \begin{smallmatrix}
        \rule{0pt}{2.2ex}
        D_{11, 21} & D_{12, 21} \\
        D_{21, 21} & D_{21, 21}
        \rule[-1.2ex]{0pt}{0pt}
      \end{smallmatrix}
      \hspace{0.8em} &
      \hspace{0.3em}
      \begin{smallmatrix}
        \rule{0pt}{2.2ex}
        D_{11, 22} & D_{12, 22} \\
        D_{21, 22} & D_{21, 22}
        \rule[-1.2ex]{0pt}{0pt}
      \end{smallmatrix}
      \hspace{0.3em}
    \end{bmatrix}
    =
    \begin{bmatrix}
      a & c & 0 & 0 \\
      e & g & 0 & 0 \\
      0 & 0 & b & d \\
      0 & 0 & f & h
    \end{bmatrix}
  \]
  Notice that the resulting matrix has become block-diagonal, whereas $\K$ had a bandwidth of 2.
\end{example}

\begin{example}
  Now endow each block $D_{ij}$ with an upper off-diagonal entry:
  \[
    D_{11} = \begin{bmatrix} a & \alpha \\ 0 & b \end{bmatrix}, \quad
    D_{12} = \begin{bmatrix} c & \beta \\ 0 & d \end{bmatrix}, \quad
    D_{21} = \begin{bmatrix} e & \delta \\ 0 & f \end{bmatrix}, \quad
    D_{22} = \begin{bmatrix} g & \gamma \\ 0 & h \end{bmatrix}.
  \]
  The full $4\times4$ matrix is
  \[
    \K = \begin{bmatrix}
      D_{11} & D_{12} \\
      D_{21} & D_{22}
    \end{bmatrix} =
    \begin{bmatrix}
      a & \alpha & c & \beta  \\
      0 & b      & 0 & d      \\
      e & \delta & g & \gamma \\
      0 & f      & 0 & h
    \end{bmatrix}.
  \]

  After applying the same permutation $\Pi$ as before, we obtain
  \[
    \Pi \K \Pi^\top =
    \begin{bmatrix}
      a & c & \alpha & \beta  \\
      e & g & \delta & \gamma \\
      0 & 0 & b      & d      \\
      0 & 0 & f      & h
    \end{bmatrix}.
  \]
  The off-diagonal entries in each block $D_{ij}$ are collected in
  the upper-right block of the permuted matrix. The permuted matrix
  is block-banded with bandwidth 2, with a dense upper-right block and a sparse
  lower-left block.
\end{example}

\end{document}